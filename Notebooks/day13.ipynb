{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa4b6f4-7a5b-49e3-9b33-c6c296dd15de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DAY 13: Model Comparison & Spark ML Pipelines ===\n\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# DAY 13: Model Comparison & Spark ML Pipelines\n",
    "# =============================================\n",
    "\n",
    "# Setup\n",
    "spark.sql(\"USE CATALOG ecommerce_prod\")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=== DAY 13: Model Comparison & Spark ML Pipelines ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd39ad05-6fa8-4bd2-bb78-5c2383a16663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1: Create ML Catalog & Volume (Unity Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644aa4a6-8926-4ecb-b64d-2a9fba0e09b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CREATING ML CATALOG & VOLUME (Unity Catalog)\n✓ Created ml_catalog.ml_schema\n✓ Created volume: ml_schema.mlflow_tmp\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"1. CREATING ML CATALOG & VOLUME (Unity Catalog)\")\n",
    "\n",
    "# Create ML-specific catalog and schema\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS ml_catalog\")\n",
    "spark.sql(\"USE CATALOG ml_catalog\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ml_schema\")\n",
    "\n",
    "# Create volume for MLflow temporary storage\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS ml_schema.mlflow_tmp\")\n",
    "\n",
    "print(\"✓ Created ml_catalog.ml_schema\")\n",
    "print(\"✓ Created volume: ml_schema.mlflow_tmp\")\n",
    "\n",
    "# Switch back to ecommerce catalog for data\n",
    "spark.sql(\"USE CATALOG ecommerce_prod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f23ce0-21e9-420f-b8dd-8b647028b91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2: Load & Prepare Training Data (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2640c647-0783-4a7d-a751-3eb8d001d1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. PREPARING TRAINING DATA (Scikit-learn)\n✓ Total samples: 206,876\n✓ Features: views, cart_adds\n✓ Target: purchases\n✓ Training samples: 165,500\n✓ Test samples: 41,376\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. PREPARING TRAINING DATA (Scikit-learn)\")\n",
    "\n",
    "# Load product ML features\n",
    "df = spark.table(\"gold.product_ml_features\").toPandas()\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[[\"views\", \"cart_adds\"]]\n",
    "y = df[\"purchases\"]\n",
    "\n",
    "print(f\"✓ Total samples: {len(df):,}\")\n",
    "print(f\"✓ Features: views, cart_adds\")\n",
    "print(f\"✓ Target: purchases\")\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✓ Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"✓ Test samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "# Convert to float64 to avoid MLflow schema warnings\n",
    "X_train_f = X_train.astype(\"float64\")\n",
    "X_test_f = X_test.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0223f5-90ea-4ec8-ac76-37fce76e08ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3: MLflow Setup for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abdf12d-f87f-4684-8dc0-643474f3387b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n3. SETTING UP MLFLOW EXPERIMENT\n✓ MLflow experiment set: /Shared/Day13_Model_Comparison\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. SETTING UP MLFLOW EXPERIMENT\")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/Day13_Model_Comparison\")\n",
    "print(\"✓ MLflow experiment set: /Shared/Day13_Model_Comparison\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dd7d9b-8ff7-4936-97cb-43cee55cf68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4: Train & Compare 3 Scikit-learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3363b966-b137-40c9-a9dc-c162218b4a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n4. TRAINING & COMPARING 3 MODELS (Scikit-learn)\n\nTraining LinearRegression...\n✓ LinearRegression R² Score: 0.9725\n\nTraining DecisionTree...\n✓ DecisionTree R² Score: 0.8380\n\nTraining RandomForest...\n✓ RandomForest R² Score: 0.9670\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n4. TRAINING & COMPARING 3 MODELS (Scikit-learn)\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and log each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        if model_name == \"DecisionTree\":\n",
    "            mlflow.log_param(\"max_depth\", 5)\n",
    "        elif model_name == \"RandomForest\":\n",
    "            mlflow.log_param(\"n_estimators\", 100)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_f, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        r2_score = model.score(X_test_f, y_test)\n",
    "        mlflow.log_metric(\"r2_score\", r2_score)\n",
    "        results[model_name] = r2_score\n",
    "        \n",
    "        # Create signature for better MLflow tracking\n",
    "        input_example = X_train_f.iloc[:5]\n",
    "        predictions = model.predict(input_example)\n",
    "        signature = infer_signature(input_example, predictions)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            artifact_path=f\"{model_name.lower()}_model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ {model_name} R² Score: {r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1380323d-3b75-45fd-b4d8-cd684b8d69ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5: Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b534365b-736d-4bff-adb2-71f11bc4c861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n5. MODEL COMPARISON RESULTS\n==================================================\nModel                R² Score        Training Time  \n--------------------------------------------------\nLinearRegression     0.9725          -              \nDecisionTree         0.8380          -              \nRandomForest         0.9670          -              \n==================================================\n✓ Best Model: LinearRegression (R²: 0.9725)\n==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Model':<20} {'R² Score':<15} {'Training Time':<15}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Simple performance summary \n",
    "best_model = max(results, key=results.get)\n",
    "best_score = results[best_model]\n",
    "\n",
    "for model_name, score in results.items():\n",
    "    print(f\"{model_name:<20} {score:<15.4f} {'-':<15}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"✓ Best Model: {best_model} (R²: {best_score:.4f})\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2524f73b-11e7-4009-930a-495a4ff3dff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6: Build Spark ML Pipeline (Scalable Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7485a8-2492-486e-9be3-df2d461793d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n6. BUILDING SPARK ML PIPELINE (Scalable)\n✓ Spark DataFrame loaded: 206,876 rows\n✓ Training samples: 165,630\n✓ Test samples: 41,246\n✓ Spark ML Pipeline trained successfully\n✓ Spark Linear Regression R²: 0.9908\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. BUILDING SPARK ML PIPELINE (Scalable)\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression as SparkLR\n",
    "\n",
    "# Load data as Spark DataFrame\n",
    "spark_df = spark.table(\"gold.product_ml_features\")\n",
    "\n",
    "# Cast to double for ML algorithms\n",
    "spark_df = spark_df.select(\n",
    "    F.col(\"product_id\"),\n",
    "    F.col(\"views\").cast(\"double\").alias(\"views\"),\n",
    "    F.col(\"cart_adds\").cast(\"double\").alias(\"cart_adds\"),\n",
    "    F.col(\"purchases\").cast(\"double\").alias(\"purchases\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark DataFrame loaded: {spark_df.count():,} rows\")\n",
    "\n",
    "# Build pipeline\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"views\", \"cart_adds\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "spark_lr = SparkLR(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"purchases\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, spark_lr])\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"✓ Training samples: {train_df.count():,}\")\n",
    "print(f\"✓ Test samples: {test_df.count():,}\")\n",
    "\n",
    "# Train pipeline\n",
    "spark_model = pipeline.fit(train_df)\n",
    "print(\"✓ Spark ML Pipeline trained successfully\")\n",
    "\n",
    "# Evaluate Spark model\n",
    "spark_predictions = spark_model.transform(test_df)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"purchases\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "spark_r2 = evaluator.evaluate(spark_predictions)\n",
    "print(f\"✓ Spark Linear Regression R²: {spark_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ef29dc-e0f7-4c0b-a702-33e66a663966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7: Log Spark Model to MLflow (Serverless-Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc00fcb-cb46-4b7f-9e94-9ee29cd88b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n7. LOGGING SPARK MODEL TO MLFLOW\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/22 01:44:58 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/22 01:45:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-1c3be76c-9c93-4611-b745-75/tmp_zxj1vqd/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n2026/01/22 01:45:00 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"dataframe_split\": {\n    \"columns\": [\n      \"views\",\n      \"cart_adds\"\n    ],\n    \"data\": [\n      [\n        178676.0,\n        10642.0\n      ],\n      [\n        6734.0,\n        43.0\n      ],\n      [\n        269.0,\n        1.0\n      ],\n      [\n        1107.0,\n        19.0\n      ],\n      [\n        2664.0,\n        169.0\n      ],\n      [\n        881.0,\n        17.0\n      ],\n      [\n        714.0,\n        26.0\n      ],\n      [\n        334.0,\n        12.0\n      ],\n      [\n        179.0,\n        5.0\n      ],\n      [\n        1668.0,\n        58.0\n      ],\n      [\n        868.0,\n        3.0\n      ],\n      [\n        1915.0,\n        45.0\n      ],\n      [\n        3064.0,\n        53.0\n      ],\n      [\n        56.0,\n        0.0\n      ],\n      [\n        19.0,\n        0.0\n      ],\n      [\n        295.0,\n        3.0\n      ],\n      [\n        1475.0,\n        60.0\n      ],\n      [\n        992.0,\n        12.0\n      ],\n      [\n        402.0,\n        2.0\n      ],\n      [\n        74.0,\n        1.0\n      ]\n    ]\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: UC volume path must be provided to save, log or load SparkML models in Databricks shared or serverless clusters. Specify environment variable 'MLFLOW_DFS_TMP' or 'dfs_tmpdir' argument that uses a UC volume path starting with '/Volumes/...' when saving, logging or loading a model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark ML Pipeline logged to MLflow\n✓ Using Unity Catalog volume for temporary storage\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7. LOGGING SPARK MODEL TO MLFLOW\")\n",
    "\n",
    "# Create sample for signature inference\n",
    "sample = spark_df.select(\n",
    "    F.col(\"views\").cast(\"double\"),\n",
    "    F.col(\"cart_adds\").cast(\"double\")\n",
    ").limit(20)\n",
    "\n",
    "# Get predictions for sample\n",
    "pred = spark_model.transform(sample).select(\"prediction\")\n",
    "\n",
    "# Infer signature\n",
    "signature = infer_signature(sample.toPandas(), pred.toPandas())\n",
    "\n",
    "# Log to MLflow with Unity Catalog volume\n",
    "with mlflow.start_run(run_name=\"Spark_LR_Pipeline\"):\n",
    "    mlflow.log_param(\"model_type\", \"SparkLinearRegressionPipeline\")\n",
    "    mlflow.log_param(\"maxIter\", 10)\n",
    "    mlflow.log_param(\"regParam\", 0.01)\n",
    "    mlflow.log_metric(\"r2_score\", spark_r2)\n",
    "    \n",
    "    mlflow.spark.log_model(\n",
    "        spark_model,\n",
    "        artifact_path=\"spark_pipeline_model\",\n",
    "        signature=signature,\n",
    "        input_example=sample.toPandas(),\n",
    "        dfs_tmpdir=\"/Volumes/ml_catalog/ml_schema/mlflow_tmp\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Spark ML Pipeline logged to MLflow\")\n",
    "    print(\"✓ Using Unity Catalog volume for temporary storage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb1c247-75ab-46e0-b0de-d85b36ede9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8: Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c55d2bd-b45e-46d3-9229-a44c134e43a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n8. FINAL MODEL COMPARISON\n============================================================\nModel Type                Framework       R² Score       \n------------------------------------------------------------\nLinearRegression          Scikit-learn    0.9725         \nDecisionTree              Scikit-learn    0.8380         \nRandomForest              Scikit-learn    0.9670         \nSparkLinearRegression     Spark ML        0.9908         \n============================================================\n✓ Overall Best Model: SparkLinearRegression (R²: 0.9908)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n8. FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model Type':<25} {'Framework':<15} {'R² Score':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Add Spark model to comparison\n",
    "results[\"SparkLinearRegression\"] = spark_r2\n",
    "\n",
    "for model_name, score in results.items():\n",
    "    framework = \"Scikit-learn\" if model_name != \"SparkLinearRegression\" else \"Spark ML\"\n",
    "    print(f\"{model_name:<25} {framework:<15} {score:<15.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine best overall model\n",
    "overall_best = max(results, key=results.get)\n",
    "print(f\"✓ Overall Best Model: {overall_best} (R²: {results[overall_best]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e50e91-7331-494f-bea3-3234ce9bbd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e746fd-923e-470b-b9ee-805350741eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nDAY 13 COMPLETED: Model Comparison & Spark ML Pipelines\n============================================================\n✓ 1. Created ML catalog & volume in Unity Catalog\n✓ 2. Prepared training data (scikit-learn)\n✓ 3. Set up MLflow experiment for comparison\n✓ 4. Trained & compared 3 scikit-learn models\n✓ 5. Built scalable Spark ML pipeline\n✓ 6. Logged Spark model to MLflow with UC volume\n✓ 7. Compared all 4 models performance\n✓ 8. Best model identified: SparkLinearRegression\n============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DAY 13 COMPLETED: Model Comparison & Spark ML Pipelines\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ 1. Created ML catalog & volume in Unity Catalog\")\n",
    "print(\"✓ 2. Prepared training data (scikit-learn)\")\n",
    "print(\"✓ 3. Set up MLflow experiment for comparison\")\n",
    "print(\"✓ 4. Trained & compared 3 scikit-learn models\")\n",
    "print(\"✓ 5. Built scalable Spark ML pipeline\")\n",
    "print(\"✓ 6. Logged Spark model to MLflow with UC volume\")\n",
    "print(\"✓ 7. Compared all 4 models performance\")\n",
    "print(\"✓ 8. Best model identified: {}\".format(overall_best))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2985340f-d373-4ec0-ab33-affc43117ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+\n|   views|cart_adds|purchases|        prediction|\n+--------+---------+---------+------------------+\n|178676.0|  10642.0|   3663.0|4454.9553568537885|\n|  6734.0|     43.0|    100.0| 9.271401167031895|\n|   269.0|      1.0|      3.0|0.3424399207831458|\n|  1107.0|     19.0|      7.0| 7.072282776367371|\n|  2664.0|    169.0|     55.0|  71.3052441277684|\n+--------+---------+---------+------------------+\n\n\nPrediction Range: -44.93 to 57465.28\nAverage Prediction: 7.99\nActual Purchase Range: 0 to 61265\n"
     ]
    }
   ],
   "source": [
    "# Check if predictions are reasonable\n",
    "sample_results = spark_model.transform(spark_df.limit(5))\n",
    "sample_results.select(\"views\", \"cart_adds\", \"purchases\", \"prediction\").show()\n",
    "\n",
    "# Check prediction range\n",
    "predictions = spark_model.transform(spark_df)\n",
    "pred_stats = predictions.select(\n",
    "    F.min(\"prediction\").alias(\"min_pred\"),\n",
    "    F.max(\"prediction\").alias(\"max_pred\"),\n",
    "    F.avg(\"prediction\").alias(\"avg_pred\")\n",
    ").collect()\n",
    "\n",
    "print(f\"\\nPrediction Range: {pred_stats[0]['min_pred']:.2f} to {pred_stats[0]['max_pred']:.2f}\")\n",
    "print(f\"Average Prediction: {pred_stats[0]['avg_pred']:.2f}\")\n",
    "print(f\"Actual Purchase Range: {y.min()} to {y.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b0f11d-6f7a-42bb-aee7-53a7c166e02f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}